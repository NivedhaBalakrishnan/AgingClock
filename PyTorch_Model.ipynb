{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd59bcab-31d2-4703-8da3-69de1891417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c27b3206-e3fd-494a-89b6-fcf0e50521aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Eve(optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Implements Eve Algorithm, proposed in `IMPROVING STOCHASTIC GRADIENT DESCENT WITH FEEDBACK`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999, 0.999), eps=1e-8,\n",
    "                 k=0.1, K=10, weight_decay=0):\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        k=k, K=K, weight_decay=weight_decay)\n",
    "        super(Eve, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure):\n",
    "        \"\"\"\n",
    "        :param closure: closure returns loss. see http://pytorch.org/docs/optim.html#optimizer-step-closure\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        loss = closure()\n",
    "        _loss = loss.item()  # float\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['m_t'] = grad.new().resize_as_(grad).zero_()\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['v_t'] = grad.new().resize_as_(grad).zero_()\n",
    "                    # f hats, smoothly tracked objective functions\n",
    "                    # \\hat{f}_0 = f_0\n",
    "                    state['ft_2'], state['ft_1'] = _loss, None\n",
    "                    state['d'] = 1\n",
    "\n",
    "                m_t, v_t = state['m_t'], state['v_t']\n",
    "                beta1, beta2, beta3 = group['betas']\n",
    "                k, K = group['k'], group['K']\n",
    "                d = state['d']\n",
    "                state['step'] += 1\n",
    "                t = state['step']\n",
    "                # initialization of \\hat{f}_1\n",
    "                if t == 1:\n",
    "                    # \\hat{f}_1 = f_1\n",
    "                    state['ft_1'] = _loss\n",
    "                # \\hat{f_{t-1}}, \\hat{f_{t-2}}\n",
    "                ft_1, ft_2 = state['ft_1'], state['ft_2']\n",
    "                # f(\\theta_{t-1})\n",
    "                f = _loss\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                m_t.mul_(beta1).add_(grad, alpha=1-beta1)\n",
    "                v_t.mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
    "\n",
    "                m_t_hat = m_t / (1 - beta1 ** t)\n",
    "                v_t_hat = v_t / (1 - beta2 ** t)\n",
    "\n",
    "                if t > 1:\n",
    "                    if f >= state['ft_2']:\n",
    "                        delta = k + 1\n",
    "                        Delta = K + 1\n",
    "                    else:\n",
    "                        delta = 1 / (K + 1)\n",
    "                        Delta = 1 / (k + 1)\n",
    "\n",
    "                    c = min(max(delta, f / ft_2), Delta)\n",
    "                    r = abs(c - 1) / min(c, 1)\n",
    "                    state['ft_1'], state['ft_2'] = c * ft_2, ft_1\n",
    "                    state['d'] = beta3 * d + (1 - beta3) * r\n",
    "\n",
    "                # update parameters\n",
    "                p.data.addcdiv_(m_t_hat,\n",
    "                                v_t_hat.sqrt().add_(group['eps']),\n",
    "                                value=-group['lr']/state['d'])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7f1cca-a3a6-4d15-aa78-aab448022ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path.cwd()/'models'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7442ac-437f-4773-9671-509b3dd4bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, target_transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.df.drop(labels='AGE', axis=1).iloc[idx]\n",
    "        x = torch.from_numpy(x.values)\n",
    "        #returning y as a scalar might be a problem\n",
    "        y = self.df.iloc[idx].AGE\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(y)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3608f723-e1a8-4b8b-829d-095664aa6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, layers, ps=0.35, in_features=20, y_range=(20, 90)):\n",
    "        super(NN, self).__init__()\n",
    "        self.y_range = y_range\n",
    "        layers = [in_features] + layers\n",
    "        layers = list(zip(layers, layers[1:]))\n",
    "        \n",
    "        l = []\n",
    "        for layer in layers:\n",
    "            l.append(nn.Linear(*layer))\n",
    "            #TODO: play with negative slope koef. of LeakyReLU\n",
    "            l.append(nn.LeakyReLU())\n",
    "            l.append(nn.Dropout(ps))\n",
    "        l.append(nn.Linear(layers[-1][1], 1))\n",
    "\n",
    "        self.arch = nn.Sequential(*l)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.arch(x)\n",
    "        x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92aa8b38-b226-4d8c-80d2-c77bdbd62c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, in_5_range = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device).float(), y.to(device).float().unsqueeze(dim=1)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            in_5_range += (abs(pred - y) < 5).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    in_5_range /= size\n",
    "    print(f\"Test Error: \\n Predictions in 5 range: {(100*in_5_range):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef5cc239-9806-4c32-bf5c-649101dbc167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device).float(), y.to(device).float().unsqueeze(dim=1)\n",
    "        if isinstance(optimizer, Eve):\n",
    "            loss = optimizer.step(closure)\n",
    "        else:\n",
    "            loss = closure()\n",
    "            optimizer.step()\n",
    "\n",
    "        if batch % 100 == 99:\n",
    "            loss, current = loss.item(), batch * len(X) + 1\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36b6aea8-d2cc-4ae4-b08d-83b03ed45228",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Data/train_data.csv')\n",
    "test_df = pd.read_csv('Data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a95f54d9-c00e-4d93-bf9f-d6a231dfd027",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(train_df)\n",
    "test_set = CustomDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f769efad-67d0-4660-9dd6-ca760b9bac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "741c426d-ad78-4e5a-a84c-d548b5e2e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [400, 200, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f872a042-9037-42b1-b993-6388e0d3241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b921e90-45e9-48ff-8152-586f990edf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "sgd_optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "eve_optimizer = Eve(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81e223ce-c1c0-4964-9130-d4bce24071ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 12.546995  [ 3169/20081]\n",
      "loss: 11.540640  [ 6369/20081]\n",
      "loss: 11.078890  [ 9569/20081]\n",
      "loss: 12.337844  [12769/20081]\n",
      "loss: 12.431058  [15969/20081]\n",
      "loss: 9.272524  [19169/20081]\n",
      "Test Error: \n",
      " Predictions in 5 range: 30.8%, Avg loss: 10.418320 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 11.538239  [ 3169/20081]\n",
      "loss: 11.206959  [ 6369/20081]\n",
      "loss: 10.019969  [ 9569/20081]\n",
      "loss: 12.314325  [12769/20081]\n",
      "loss: 13.801285  [15969/20081]\n",
      "loss: 11.273022  [19169/20081]\n",
      "Test Error: \n",
      " Predictions in 5 range: 28.9%, Avg loss: 10.532594 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, eve_optimizer)\n",
    "    test(test_dataloader, model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
